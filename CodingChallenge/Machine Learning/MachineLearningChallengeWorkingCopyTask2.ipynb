{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Challenge\n",
    "\n",
    "Below are 2 data challenges that test for your ability to:\n",
    "- Wrangle/clean data to make it usable by a model\n",
    "- Figure out how to set up X's and y's for a use case, given a dataset\n",
    "- Write code to robustly and reproducibly preprocess data\n",
    "- Pick/design the right model, and tune hyperparameters to get the best performance\n",
    "\n",
    "You can use any programming language, model, and package to solve these problems. Let us know of any assumptions you make in your process.\n",
    "\n",
    "#### Deliverables:\n",
    "- A link to a github repository that contains:\n",
    "    - Clearly commented code that was written to solve these problems\n",
    "    - Your trained models stored in a file (`.pkl`, `.h5`, `.tar` - whatever is appropriate). The models must have `predict(X)` functions. \n",
    "    - A readme file that contains:\n",
    "        - Instructions to easily access/load the above\n",
    "        - A writeup explaining any significant design decisions and your reasons for making them. \n",
    "        - If needed, a brief writeup explaining anything you are particularly proud of in your implementation that you might want us to focus on\n",
    "\n",
    "#### How we'll assess your work:\n",
    "- Accuracy/RMSE of your model when predicting on held-out data\n",
    "- How well various edge cases are handled when testing on held-out data. For example, if the held-out data contains:\n",
    "    - A new column that wasn't present in the dataset given to you\n",
    "    - New value in a categorical field that wasn't seen in the dataset given to you\n",
    "    - NA values\n",
    "- Efficiency of the code. \n",
    "    - Is it easy to understand? \n",
    "    - Are the variable names descriptive? \n",
    "    - Are there any variables created that aren't used? \n",
    "    - Is redundant code replaced with function calls? \n",
    "    - Is vectorized implementation used instead of nested for loops? \n",
    "    - Are classes defined and objects created where applicable? \n",
    "    - Are packages used to perform tasks instead of implementing them from scratch?\n",
    "    \n",
    "**NOTE:** Your stored models, once loaded, should *just work* when fed with our held-out data (which looks similar to the data we've given you). We won't do any preprocessing before we feed it into the model's `predict(X)` function; `predict(X)` should handle the preprocessing. Pay particular attention to handling the edge cases we've talked about.\n",
    "\n",
    "Feel free to ask questions to clarify things. Submit everything you tried, not just the things that worked. I encourage you to try and showcase your talents. The more you go above and beyond what's expected, the more impressed we'll be. **Bonus points if you fit Keras/Tensorflow/Pytorch/Caffe models** in addition to your Linear/Tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakash\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing as scale\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from math import sqrt\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, average_precision_score, precision_score, f1_score,recall_score, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used in Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(data):\n",
    "    \"\"\"\n",
    "    Function to preprocess similar datasets: \n",
    "    Takes in a dataframe, checks for null values, replaces categorical value columns with dummy variables\n",
    "    and fills the remaining null values in the numerical columns with the means of that column\"\"\"\n",
    "    \n",
    "    df = data\n",
    "    df.fillna(method='ffill', inplace=True)         #As the data is arranged chronologically, we fill the next missing variable with that of the previous hour/day\n",
    "    df.fillna(method='bfill', inplace=True)         #Incase some NaNs are at the start\n",
    "    \n",
    "    categorical_columns = df.select_dtypes(include=['object'])\n",
    "    dummy_columns = pd.get_dummies(categorical_columns)\n",
    "    \n",
    "    df = pd.concat([df.drop(categorical_columns, axis=1), dummy_columns], axis=1)\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    \"\"\"\n",
    "    Function that takes in a dataset with numerical values and standardizes it\n",
    "    \"\"\"\n",
    "    standard_sc = scale.StandardScaler()\n",
    "    x_std = standard_sc.fit_transform(df)\n",
    "    df_scaled = pd.DataFrame(x_std)\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "`forecasting_dataset.csv` is a file that contains pollution data for a city. Your task is to create a model that, when fed with columns `co_gt`, `nhmc`, `c6h6`, `s2`, `nox`, `s3`, `no2`, `s4`, `s5`, `t`, `rh`, `ah`, and `level`, predicts the value of `y` six hours later.\n",
    "\n",
    "**NOTE:** In the data we've given you, the value of `y` for a given row is the value of `y` *for the timestamp of that same row*. We're asking you to predict the value of `y` 6 hours *after the timestamp of that row*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>co_gt</th>\n",
       "      <th>nhmc</th>\n",
       "      <th>c6h6</th>\n",
       "      <th>s2</th>\n",
       "      <th>nox</th>\n",
       "      <th>s3</th>\n",
       "      <th>no2</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>t</th>\n",
       "      <th>rh</th>\n",
       "      <th>ah</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>867.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>57.3</td>\n",
       "      <td>0.9603</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>704.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>861.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1603.0</td>\n",
       "      <td>860.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.9612</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.7</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>1386.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>626.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>2138.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.3</td>\n",
       "      <td>38.6</td>\n",
       "      <td>1.0919</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.1</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>952.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>27.3</td>\n",
       "      <td>1.0479</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>21.7</td>\n",
       "      <td>1342.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>1546.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>54.1</td>\n",
       "      <td>0.8003</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   co_gt   nhmc  c6h6      s2    nox     s3    no2      s4      s5     t  \\\n",
       "0 -200.0 -200.0   7.2   867.0 -200.0  834.0 -200.0  1314.0   891.0  14.8   \n",
       "1    0.5 -200.0   3.9   704.0 -200.0  861.0 -200.0  1603.0   860.0  24.4   \n",
       "2    3.7 -200.0  23.3  1386.0    NaN  626.0  109.0  2138.0     NaN  23.3   \n",
       "3    2.1 -200.0  12.1  1052.0  183.0  779.0    NaN  1690.0   952.0  28.5   \n",
       "4    4.4 -200.0  21.7  1342.0  786.0  499.0  206.0  1546.0  2006.0  12.9   \n",
       "\n",
       "     rh      ah level  \n",
       "0  57.3  0.9603   NaN  \n",
       "1  65.0  1.9612   Low  \n",
       "2  38.6  1.0919  High  \n",
       "3  27.3  1.0479  High  \n",
       "4  54.1  0.8003  High  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## What the data that we'll feed into your model's predict(X) function will look like:\n",
    "# Notice what the level column looks like\n",
    "pd.read_csv(\"forecasting_dataset.csv\").drop(labels=['date', 'time', 'y'], axis='columns').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leaving the dates and time as separate columns doesn't sort well, so let's parse them, sort and then drop! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"forecasting_dataset.csv\", parse_dates=[['date','time']]).sort_values(by = ['date_time'])\n",
    "df.drop('date_time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same preprocess function we used in the first task to fill the NaNs with adjacent values and create dummy variable columns using native pandas functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preProcess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to task 1 we can convert the time series dataset to that of supervised learning. \n",
    "We need to predict the y value, given a set of parameters, 6 hours later. \n",
    "\n",
    "So we can simply shift the dataset to align the y values 6 hours later to the parameters in the present timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y_6_hours_later'] = df.y.shift(-6)\n",
    "df.drop('y', axis=1, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into X and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df.iloc[:,:-1]\n",
    "df_y = df.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_6_hours_later</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3974</th>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6374</th>\n",
       "      <td>1136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>1094.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937</th>\n",
       "      <td>1010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8292</th>\n",
       "      <td>1011.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_6_hours_later\n",
       "3974           1185.0\n",
       "6374           1136.0\n",
       "883            1094.0\n",
       "5937           1010.0\n",
       "8292           1011.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split the data into training, validation, and testing sets. From here onwards we forget that the test set exists. \n",
    "\n",
    "We use just the validation set to determine the model and tune its hyperparameters. Finally at the end once all factors are decided we find out the test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(df_X, df_y, test_size = 0.4, random_state = 19 )\n",
    "xval, xtest, yval, ytest = train_test_split(xtest, ytest, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>co_gt</th>\n",
       "      <th>nhmc</th>\n",
       "      <th>c6h6</th>\n",
       "      <th>s2</th>\n",
       "      <th>nox</th>\n",
       "      <th>s3</th>\n",
       "      <th>no2</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>t</th>\n",
       "      <th>rh</th>\n",
       "      <th>ah</th>\n",
       "      <th>level_High</th>\n",
       "      <th>level_Low</th>\n",
       "      <th>level_Moderate</th>\n",
       "      <th>level_Very High</th>\n",
       "      <th>level_Very low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>1.3</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>829.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>621.0</td>\n",
       "      <td>26.1</td>\n",
       "      <td>21.1</td>\n",
       "      <td>0.7023</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>0.7</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>707.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1325.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>40.2</td>\n",
       "      <td>17.7</td>\n",
       "      <td>1.2989</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6433</th>\n",
       "      <td>0.9</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>718.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>981.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>56.8</td>\n",
       "      <td>1.0470</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8038</th>\n",
       "      <td>1.3</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>731.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>815.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>0.4523</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5435</th>\n",
       "      <td>-200.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>885.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1588.0</td>\n",
       "      <td>974.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>66.1</td>\n",
       "      <td>1.0236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      co_gt   nhmc  c6h6     s2    nox      s3    no2      s4      s5     t  \\\n",
       "2825    1.3 -200.0   6.4  829.0   82.0  1124.0   75.0  1333.0   621.0  26.1   \n",
       "1102    0.7 -200.0   3.9  707.0   63.0  1063.0   85.0  1325.0   417.0  40.2   \n",
       "6433    0.9 -200.0   4.1  718.0  106.0   981.0   71.0  1223.0   872.0  16.3   \n",
       "8038    1.3 -200.0   4.4  731.0  272.0   815.0  149.0   936.0  1018.0   1.0   \n",
       "5435 -200.0 -200.0   8.5  885.0 -200.0   884.0 -200.0  1588.0   974.0  11.0   \n",
       "\n",
       "        rh      ah  level_High  level_Low  level_Moderate  level_Very High  \\\n",
       "2825  21.1  0.7023           1          0               0                0   \n",
       "1102  17.7  1.2989           0          1               0                0   \n",
       "6433  56.8  1.0470           0          1               0                0   \n",
       "8038  67.7  0.4523           1          0               0                0   \n",
       "5435  66.1  1.0236           0          0               0                0   \n",
       "\n",
       "      level_Very low  \n",
       "2825               0  \n",
       "1102               0  \n",
       "6433               0  \n",
       "8038               0  \n",
       "5435               1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we'll train with three models, linear regression, random forest and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegression(xtrain,xtest, ytrain, ytest):\n",
    "    LR = LinearRegression()\n",
    "    model = LR.fit(xtrain,ytrain)\n",
    "    score = LR.score(xtest, ytest)\n",
    "    pred = model.predict(xtest)\n",
    "    print(\"R^2 score: \", (score))\n",
    "    print(\"Coefficients of the model: \", model.coef_)\n",
    "    \n",
    "    print('\\nRMSE:', sqrt(mean_squared_error(ytest,pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForestRegressor(xtrain,xtest, ytrain, ytest):\n",
    "    LR = RandomForestRegressor() #SPOILER!: the tuned parameters -> bootstrap = True, max_depth = 70, min_samples_leaf = 1, min_samples_split = 2, n_estimators = 1400 )\n",
    "    model = LR.fit(xtrain,ytrain)\n",
    "    score = LR.score(xtest, ytest)\n",
    "    pred = model.predict(xtest)\n",
    "    #print(model)\n",
    "    print(\"R^2 score: \", (score))\n",
    "    #print(pred)\n",
    "    print(\"\\nRMSE:\", sqrt(mean_squared_error(ytest,pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBRegression(xtrain,xtest, ytrain, ytest):\n",
    "    XGB = XGBRegressor()\n",
    "    model = XGB.fit(xtrain,ytrain)\n",
    "    score = XGB.score(xtest, ytest)\n",
    "    pred = model.predict(xtest)\n",
    "    print(\"R2 score\",(score))\n",
    "    #print(pred)\n",
    "    print(\"\\nRMSE:\", sqrt(mean_squared_error(ytest,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score:  0.4374621144731929\n",
      "Coefficients of the model:  [[ 3.83556912e-01  2.36019205e-01 -1.47677657e+00 -2.26211659e-01\n",
      "   1.75088855e-01  5.57624178e-02 -3.40337098e-01 -4.52689167e-02\n",
      "   3.29726856e-01  6.50345551e+00  2.26607814e+00 -3.88754555e+00\n",
      "  -1.46289951e+01  4.88009047e+01  1.93546516e+00 -1.32297519e+02\n",
      "   9.61901441e+01]]\n",
      "\n",
      "RMSE: 250.45362050999765\n"
     ]
    }
   ],
   "source": [
    "linearRegression(xtrain,xval, ytrain, yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score 0.48476455945239566\n",
      "\n",
      "RMSE: 239.69241445454526\n"
     ]
    }
   ],
   "source": [
    "XGBRegression(xtrain,xval,ytrain,yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakash\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score:  0.49462090364806693\n",
      "\n",
      "RMSE: 237.38871150957598\n"
     ]
    }
   ],
   "source": [
    "randomForestRegressor(xtrain, xval, ytrain, yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest gives us the highest r^2 score as well as the lowest error so we can choose to tune the hyperparameters for it. A useful method is the randomSearchCV function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Parameters: \n",
      "\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'mse',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 10,\n",
      " 'n_jobs': 1,\n",
      " 'oob_score': False,\n",
      " 'random_state': None,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "print('Current Parameters: \\n')\n",
    "pprint(rf.get_params())  #pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our search grid: \n",
      "\n",
      "{'bootstrap': (True, False),\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "#Method of selecting samples for training each tree\n",
    "bootstrap = True, False\n",
    "\n",
    "#Max levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10,110, num=11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "#max number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "#Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1,2,4]\n",
    "\n",
    "#Minum number of samples required to split a node\n",
    "min_samples_split = [2,5,10]\n",
    "\n",
    "# Number of Trees in the random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "\n",
    "random_grid = {'bootstrap'        : bootstrap,\n",
    "               'max_depth'        : max_depth,\n",
    "               'min_samples_leaf' : min_samples_leaf,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'n_estimators':n_estimators}\n",
    "print(\"Our search grid: \\n\")\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Search Trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 200 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 353 tasks      | elapsed: 24.8min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed: 43.5min finished\n",
      "C:\\Users\\Prakash\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\sklearn\\model_selection\\_search.py:740: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=200, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'bootstrap': (True, False), 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]},\n",
       "          pre_dispatch='2*n_jobs', random_state=19, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFR = RandomForestRegressor()\n",
    "\n",
    "#Random search using 3 fold cross validation and serch across 100 combinations\n",
    "\n",
    "RFR_random = RandomizedSearchCV(estimator = rf, \n",
    "                                param_distributions = random_grid,\n",
    "                                n_iter = 200, \n",
    "                                cv = 3,\n",
    "                                verbose =2, \n",
    "                                random_state = 19,\n",
    "                                n_jobs = -1)\n",
    "\n",
    "RFR_random.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 70,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 1400}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFR_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I feel like this might overfit the data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally evaluate on the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakash\\Miniconda3\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score:  0.5940845519491698\n",
      "\n",
      "RMSE: 224.28082656117505\n"
     ]
    }
   ],
   "source": [
    "randomForestRegressor(xtrain, xtest, ytrain, ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
